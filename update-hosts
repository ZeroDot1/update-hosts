#!/usr/bin/env bash

#
#  The MIT License (MIT)
#
#  Copyright (c) 2015 - 2016 Peter Kenji Yamanaka
#  Copyright (c) 2015 Héctor Molinero Fernández
#
#  Permission is hereby granted, free of charge, to any person obtaining a copy
#  of this software and associated documentation files (the "Software"), to deal
#  in the Software without restriction, including without limitation the rights
#  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
#  copies of the Software, and to permit persons to whom the Software is
#  furnished to do so, subject to the following conditions:
#
#  The above copyright notice and this permission notice shall be included in all
#  copies or substantial portions of the Software.
#
#  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
#  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
#  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
#  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
#  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
#  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
#  SOFTWARE.

# Author:
#  Héctor Molinero Fernández <me@znt.se>.
#
# Modified by:
#  pyamsoft <pyam(dot)soft(at)gmail(dot)com>

# For more information about these particular options, see
#    https://stackoverflow.com/questions/13468481/when-to-use-set-e
set -u
set -o pipefail

# Configuration:
readonly sudolock="/tmp/update-hosts.sudolock"

temp_files=()
pids="" # Leave blank
blocklist="" # Leave this blank
hosts="" # Leave this blank

# Options
remove_backups=0
display_pager=0

readonly dst_hosts="/etc/hosts"
readonly dst_backup="${dst_hosts}.backup.$(date +%Y%m%d)"
readonly dst_ip="0.0.0.0"

# NOTE: Adding the alias of the 0 domain to itself fixes a display issue that
#       can occur when using traceroute, netstat and the like.
header="
127.0.0.1 $(uname -n) localhost
255.255.255.255 broadcasthost
0.0.0.0 0.0.0.0
"

readonly sources=(
  # The base sources
  # Provides a basic amount of protection without breaking sites, or breaking
  # only a minimal amount of sites
  "http://winhelp2002.mvps.org/hosts.txt"
  "http://www.malwaredomainlist.com/hostslist/hosts.txt"
  "http://pgl.yoyo.org/adservers/serverlist.php?hostformat=hosts&mimetype=plaintext"
  "http://adaway.org/hosts.txt"
  "http://someonewhocares.org/hosts/hosts"
  "http://malwaredomains.lehigh.edu/files/justdomains"

  # Hosts-file.net server classifications
  # Provides more in-depth protection and host blocking at the cost of
  # breaking some sites
  "http://hosts-file.net/ad_servers.txt" # Ad servers
  "http://hosts-file.net/emd.txt" # Malware sites
  "http://hosts-file.net/exp.txt" # Exploit sites
  "http://hosts-file.net/fsa.txt" # Fraud sites
  "http://hosts-file.net/hjk.txt" # Hijack sites
  "http://hosts-file.net/mmt.txt" # Misleading marketing
  "http://hosts-file.net/psh.txt" # Phishing sites

  # These entries are very large, and may break certain sites
  # that rely on CDNs or third party sources
  # Instead of downloading the entire hosts file and additional parts
  # the split download above is recommended. The parts that you do not
  # desire can be ignored individually instead of an all or nothing approach.
  # The two sources below are generally not recommended nor needed. If you do
  # wish to use them, be sure to comment out the above hosts-file.net entries
  # or you will largely be pulling duplicate sources.

  # The full host list, blocks malicious domains
  #
  # "http://hosts-file.net/download/hosts.txt"

  # Any updates to the hosts-file.net hosts file since the point release
  #
  # "http://hosts-file.net/hphosts-partial.txt"
)

# Regex allowed
#  '\.com$'   -> all domains that end with '.com'
#  '^example' -> all domains that start with 'example'
#  '^sub.\example\.org$' -> literal domain 'sub.exmaple.org'

# Due to the binary yes or no nature of hosts file, all of the domains that
# relate in some way to the uBlock0 Unbreak list will be added here.
# This is to allow uBlock0 to block the specific parts of the domains, while
# not breaking other sites that actually rely on them
# Though not a great solution, this seems to be the only way to get around
# some sites breaking when blocking certain hosts
readonly whitelist=(
  # Fixes issues in Chrome Store and other sites
  'google-analytics\.com$'

  # Fixes adf.ly
  'adf\.ly$'

  # Fixes some cdns
  '^widget-cdn\.rpxnow\.com$'

  # Fixes broken videos
  '^tags\.tiqcdn\.com$'

  # Fixes statcounter sites
  'statcounter\.com$'

  # Fixes liverail.com
  '^cdn-static\.liverail\.com$'
  '^vox-static\.liverail\.com$'

  # Fixes cnet.com videos
  '^s0\.2mdn\.net$'

  # Fixes other cdns
  '^cdn-i\.dmdentertainment\.com$'

  # Fixes other cdns
  '^cdn\.vurb\.com$'

  # Fixes target.com
  '^target\.122\.2o7\.net$'

  # Fixes fwmrm.net
  '^adm\.fwmrm\.net$'

  # Fixes mopub.com
  'mopub\.com$'

  # Fixes safelinking.net
  'safelinking\.net$'

  # Fixes outbrain
  'outbrain\.com$'

  # Fixes inc.com
  'inc\.com$'

  # Fixes flurry.com
  'flurry\.com$'

  # Fixes speedtest.net
  '^c\.speedtest\.net$'

  # Fixes boldchat
  'boldchat\.com$'

  # Fixes mixpanel
  'mixpanel\.com$'

  # Fixes turner cdn
  '^cdn\.turner\.com$'

  # Fixes yahoo! APIs
  '^yui\.yahooapis\.com$'

  # Fixes top.mail.ru
  '^top\.mail\.ru$'

  # Fixes washington post
  '^css\.washingtonpost\.com$'

  # Fix pcekspert.com
  'pcekspert\.com$'

  # Fix clickbank.com
  'clickbank\.com$'

  # Fix yandex.ru
  'yandex\.ru$'

  # Fix yastatic.net
  'yastatic\.net$'

  # Fix carbonads
  'carbonads\.net$'

  # Fix some google api thing
  '^imasdk\.googleapis\.com$'

  # Fixes 2o7.net
  '2o7\.net$'

  # Fixes quantcast
  'quantcast\.com$'

  # Chrome store
  '^css5\.gaanacdn\.com$'

  # keen github.io
  '^keen\.github\.io$'

  # Fixes sites that rely on intergi.com
  '^cdn\.phoenix\.intergi\.com$'

  # Fixes kissmetrics.com
  'kissmetrics\.com$'

  # Fixes viewpoint
  'viewpoint\.com$'

  # Fixes woopra.com
  'woopra\.com$'

  # Fixes monetate.net
  'monetate\.net$'

  # Fixes googleads.g.doubleclick.net
  '^googleads\.g\.doubleclick\.net$'

  # fixes echohosting.cafe24
  '^img\.echohosting\.cafe24\.com$'

  # Fixes googletagmanager
  'googletagmanager\.com$'

  # Fixes partnet.googleadservices.com
  '^partner\.googleadservices\.com$'

  # Fix pagefair.com
  'pagefair\.com$'

  # Fixes redfin
  '^ssl\.cdn-redfin\.com$'

  # Fixes southwest.com
  '^www\.southwest\.com$'

  # Fixes marketwatch.com
  'marketwatch\.com$'

  # Fixes cloudfront.net
  'cloudfront\.net$'

  # Fixes nationalreview.com
  'nationalreview\.com$'

  # Fixes liveperson.net issues
  'liveperson\.net$'
)

# BLACKLIST DOMAINS
# Only exact strings, no regex supported
#    agor.io: Jumpscare
#    pinion.gg: Ads in Counter Strike: Global Offensive and other games
readonly blacklist=(
  "agor.io"
  "www.agor.io"
  "adback.pinion.gg"
  "api.pinion.gg"
  "bin.pinion.gg"
  "blog.pinion.gg"
  "bork.pinion.gg"
  "calendar.pinion.gg"
  "cdn.pinion.gg"
  "cp.pinion.gg"
  "crm.pinion.gg"
  "delivery.pinion.gg"
  "direct.pinion.gg"
  "docs.pinion.gg"
  "immuniser.pinion.gg"
  "kermit.pinion.gg"
  "legacy.pinion.gg"
  "log.pinion.gg"
  "mail.pinion.gg"
  "mailer.pinion.gg"
  "motd.pinion.gg"
  "ns1.pinion.gg"
  "ns2.pinion.gg"
  "ns3.pinion.gg"
  "ns4.pinion.gg"
  "ns5.pinion.gg"
  "ns6.pinion.gg"
  "office.pinion.gg"
  "oscar.pinion.gg"
  "pinion-log.pinion.gg"
  "pinion.gg"
  "pog.pinion.gg"
  "quartermaster.pinion.gg"
  "seen.pinion.gg"
  "stage.pinion.gg"
  "templ4d2.pinion.gg"
  "tix.pinion.gg"
  "transcoded.pinion.gg"
  "video.pinion.gg"
  "voip.pinion.gg"
  "wiki.pinion.gg"
  "www.pinion.gg"
)

##
# Display a general message
##
action_msg()
{
  printf -- "%b + %b%s%b\n" "\e[1;33m" "\e[1;32m" "$1" "\e[0m"
}

##
# Display an information snippet
##
info_msg()
{
  printf -- "   - %s\n" "$1"
}

##
# Check the environment path for the either wget or curl
# exit if it is not found
##
check_for_download_ability()
{
  if ! which curl > /dev/null 2>&1; then
    if ! which wget > /dev/null 2>&1; then
      printf -- "Either wget or curl are required for this script\n"
      printf -- "Please install either 'wget' or 'curl' onto your system.\n"
      exit 1
    fi
  fi
}

##
# Check the environment path for the given binary, exit if it is not found
##
check_for_binary()
{
  if ! which "$1" > /dev/null 2>&1; then
    printf -- "The '%s' binary is required for this script.\n" "$1"
    printf -- "Please install '%s' onto your system.\n" "$1"
    exit 1
  fi
}

##
# Print out the user configuration
##
print_config()
{
  info_msg "Hosts location: ${dst_hosts}"
  info_msg "Destination IP: ${dst_ip}"
}

##
# Decide what program to DL with
##
decide_dl_client()
{
  # Check for curl, if not found, use wget, as the check for either
  # Should have passed before this function
  local cmd=
  if which curl > /dev/null 2>&1; then
    cmd="curl -fsSL --connect-timeout 40 --max-time 60"
  else
    cmd="wget -T 10 -qO-"
  fi
  printf -- "%s" "${cmd}"
  unset cmd
}

##
# Sources list parsing
##
download_to_tempfile()
{
  # $1 cmd
  # $2 url
  # $3 tmpfile
  info_msg "Downloading from $2"
  local hostfile="$(printf -- "%s" "$($1 $2)")"
  if [[ -z "${hostfile}" ]]; then
    info_msg "  Failed to download from: $2"
    return
  fi

  # Guarantee an empty file
  printf -- "%s\n" "${hostfile}" > "$3"

  # Process each tmpfile seperately
  change_eol "$3" "$2"
  select_host_lines "$3" "$2"
  remove_untrusted_domain "$3" "$2"
  normalize_to_lowercase "$3" "$2"
  remove_local_entries "$3" "$2"
}

##
# Download the hosts file from each URL specified in source
# Download runs in parallel in the background
##
download_sources()
{
  # $1 cmd
  local sources_size="${#sources[@]}"
  for ((i=0; i<sources_size; i++)); do
    temp_files[$i]="$(mktemp)"
    local url="${sources[$i]}"
    download_to_tempfile "$1" "${url}" "${temp_files[$i]}" &
    unset url
    pids="${pids} $!"
  done

  # Wait for all parallel downloads to finish
  # Instead of taking a long time in total, this will only take as long
  # as it takes for the longest download to complete
  wait ${pids}

  # Write the contents of each download to the blocklist
  for hostfile in "${temp_files[@]}"; do
    local content="$(cat ${hostfile})"
    if [[ ! -z "${content}" ]]; then
      if [[ -z "${blocklist}" ]]; then
        blocklist=$(printf -- "%s\n" "${content}")
      else
        blocklist=$(printf -- "%s\n%s\n" "${blocklist}" "${content}")
      fi
    fi

    # Remove the temp file
    rm -f "${hostfile}"
    unset content
  done
}

##
# Change EOL from DOS to Unix format
##
change_eol()
{
  # $1 file
  # $2 url
  info_msg "Change EOL to Unix format: $2 ($1)"
  sed -r -i 's/$//' "$1"
}

##
# Only select lines which contain a host, do not select comments/other
##
select_host_lines()
{
  # $1 file
  # $2 url
  info_msg "Select only hosts lines: $2 ($1)"
  local ip_regex='([0-9]{1,3}\.){3}[0-9]{1,3}[[:blank:]]+'
  local domain_regex='([[:alnum:]_-]{1,63}\.)+[[:alpha:]][[:alnum:]_-]{1,62}'

  # KLUDGE
  # This leaves trailing comments in lines, making it not as effective
  # sed -n -i -r "/(^(${ip_regex})|^)${domain_regex}/p" "$1"
  #
  # This is an ugly workaround
  local content=$(grep -oE "(^(${ip_regex})|^)${domain_regex}" "$1")
  printf -- "$content" > "$1"
  unset content
  unset ip_regex domain_regex
}

##
# Remove any predefined destinations, as they may be untrustworthy
##
remove_untrusted_domain()
{
  # $1 file
  # $2 url
  info_msg "Remove old destination: $2 ($1)"
  sed -i -r 's/^([0-9]{1,3}\.){3}[0-9]{1,3}[[:space:]]+//g' "$1"
}

##
# Normalize all entries to lowercase strings
##
normalize_to_lowercase()
{
  # $1 file
  # $2 url
  info_msg "Transform all entries to lowercase: $2 ($1)"

  # KLUDGE
  # Awk for some reason stops parsing after 1024 lines
  # awk -v tf="$1" '{print tolower($0) > tf}' "$1"
  #
  # This is an ugly work around
  local content=$(awk '{print tolower($0)}' "$1")
  printf -- "${content}" > "$1"
  unset content
}

##
# Remove any entries containing the localhost, as it will be added by
# the header later on
##
remove_local_entries()
{
  # $1 file
  # $2 url
  info_msg "Remove local entries: $2 ($1)"
  sed -n -i -r \
    '/^(localhost|localhost.localdomain|local|broadcasthost)$/!p' "$1"
}

##
# Apply the user whitelist
##
apply_whitelist()
{
  info_msg "Apply whitelist"
  if [[ -z "${blocklist}" ]]; then
    return 1
  fi

  for domain in "${whitelist[@]}"; do
    blocklist=$(printf -- "%s\n" "${blocklist}" | sed -r "/${domain}/d")
  done
}

##
# Apply the user blacklist
##
apply_blacklist()
{
  info_msg "Apply blacklist"
  # Cache the blacklist domains so that the entire block list
  # does not have to be echoed each time.
  # Fix an issue which adds a rogue newline into the blacklist,
  # creating an blank entry in the hosts file of
  # 0.0.0.0 <space>

  # Domains initialized as first blacklist entry
  local domains="${blacklist[0]}"
  # Only loop if the blacklist is multiple entries
  if [[ "${#blacklist[@]}" -gt 1 ]]; then
    for domain in "${blacklist[@]:1}"; do
      domains=$(printf -- "%s\n%s\n" "${domains}" "${domain}")
    done
  fi

  if [[ -z "${blocklist}" ]]; then
    blocklist=$(printf -- "%s\n" "${domains}")
  else
    blocklist=$(printf -- "%s\n%s\n" "${blocklist}" "${domains}")
  fi
  unset domains
}

##
# Remove duplicate entries
##
remove_duplicates()
{
  info_msg "Remove duplicate entries"
  blocklist=$(printf -- "%s\n" "${blocklist}" | awk '!a[$0]++')
}

##
# Sort the entries by string numerical value
##
sort_entries()
{
  info_msg "Sort entries"
  blocklist=$(printf -- "%s\n" "${blocklist}" | sort -n)
}

##
# Add the configured destination, by default the zero address
##
add_new_destination()
{
  info_msg "Add new destination"
  blocklist=$(printf -- "%s\n" "${blocklist}" | sed "s/^/${dst_ip} /g")
}

##
# Parse the header and create the new hosts file
##
generate_new_hosts_file()
{
  header=$(printf -- "# <header>%s# </header>\n" "${header}")
  header=$(printf -- "# %s\n%s\n" "$(date)" "${header}")
  hosts=$(printf -- "%s\n# <blocklist>\n%s\n# </blocklist>" \
    "${header}" "${blocklist}")
}

##
# Backup original hosts file
##
backup_old_hosts_file()
{
  info_msg "Creating backup of original hosts file..."
  if [[ "${EUID}" -ne 0 ]]; then
    sudo cp "${dst_hosts}" "${dst_backup}"
  else
    cp "${dst_hosts}" "${dst_backup}"
  fi
}

##
# Install the new hosts file to the destination
##
install_new_hosts_file()
{
  info_msg "Installing new hosts file..."
  if [[ "${EUID}" -ne 0 ]]; then
    printf -- "%s\n" "${hosts}" | sudo tee "${dst_hosts}" > /dev/null
  else
    printf -- "%s\n" "${hosts}" | tee "${dst_hosts}" > /dev/null
  fi
}

##
# Fixes permission on file to 644
##
fix_file_permissons()
{
  info_msg "Fixing permissions for file $1..."
  if [[ "${EUID}" -ne 0 ]]; then
    sudo chmod 644 "$1"
  else
    chmod 644 "$1"
  fi
}

##
# Print the differences between the number of hosts in each file
##
print_hosts_file_differences()
{
  local old_hosts_number=$(wc -l < "${dst_backup}")
  local new_hosts_number=$(wc -l < "${dst_hosts}")
  printf -- "%b + %b%s %bhosts added! (%b + %b%s %b)\n" \
    "\e[1;33m" "\e[1;32m" "${new_hosts_number}" \
    "\e[0m" "\e[1;32m" "\e[1;34m" \
    "$((new_hosts_number - old_hosts_number))" "\e[0m"
  unset old_hosts_number new_hosts_number
}

remove_all_backup_hosts_files()
{
  for arg in /etc/hosts.backup.*; do
    info_msg "Removing backup file: ${arg}..."
    if [[ "${EUID}" -eq 0 ]]; then
      rm -f "${arg}" > /dev/null 2>&1
    else
      sudo rm -f "${arg}" > /dev/null 2>&1
    fi
  done
}

sudoloop()
{
  touch "${sudolock}"
  while [[ -e "${sudolock}" ]]; do
    sudo -n -v
    sleep 2
  done
}

cancel_sudoloop()
{
  [[ -e "${sudolock}" ]] && rm -f "${sudolock}"

  # Kill runaway pids
  for p in ${pids}; do
    kill "${p}" > /dev/null 2>&1
  done

  # rm leftover files
  for hostfile in "${temp_files[@]}"; do
    rm -f "${hostfile}" > /dev/null 2>&1
  done

  exit 0
}


# Script begins here

# Check for required binaries, exit out if they are not present
check_for_download_ability
check_for_binary sed
check_for_binary grep
check_for_binary sort
check_for_binary touch
check_for_binary sleep
check_for_binary rm
check_for_binary awk
check_for_binary tee
check_for_binary cp
check_for_binary wc
check_for_binary printf
check_for_binary chmod
check_for_binary mktemp
check_for_binary cat

# Handle options
# Options
#   -h | --help     Display help
#   -r | --remove   Remove backups
#   -p | --pager    Display hosts file in pager
for arg in $@; do
  case "${arg}" in
    "-h"|"--help")
      action_msg "Options:"
      printf -- "     -h | --help     Display help\n"
      printf -- "     -r | --remove   Remove backups\n"
      printf -- "     -p | --pager    Display hosts file in pager\n"
      shift
      exit 0
      ;;
    "-r"|"--remove")
      remove_backups=1
      shift
      ;;
    "-p"|"--pager")
      display_pager=1
      shift;
      ;;
  esac
done

# Check if pager is set
# TODO Add support for a $PAGER variable
if [[ "${display_pager}" -eq 1 ]]; then

  # Check for the PAGER environment variable
  # Conditional assignment works when using the set -u option
  : "${PAGER:=less}"

  # Using the pager requires extra binary
  check_for_binary "${PAGER}"
  action_msg "Displaying ${dst_hosts} in ${PAGER}..."

  "${PAGER}" "${dst_hosts}"
  action_msg "Exiting..."

  # Clean exit
  exit 0
fi

# Trap exit signals
trap cancel_sudoloop INT

if [[ "${EUID}" -ne 0 ]]; then
  check_for_binary sudo

  # initialize sudo
  sudo -v || exit 1

  # Start sudoloop
  sudoloop &
fi

# Process begins:
action_msg "Configuration:"
print_config

action_msg "Downloading sources lists..."
download_sources "$(decide_dl_client)"

action_msg "Generating hosts file..."
apply_whitelist
apply_blacklist
remove_duplicates
sort_entries
add_new_destination
generate_new_hosts_file
backup_old_hosts_file
install_new_hosts_file
fix_file_permissons "${dst_hosts}"
fix_file_permissons "${dst_backup}"
print_hosts_file_differences

# Cleanup
if [[ "${remove_backups}" -eq 1 ]]; then
  remove_all_backup_hosts_files
fi
cancel_sudoloop

# vim: set syntax=sh tabstop=2 softtabstop=2 shiftwidth=2 shiftround expandtab:

